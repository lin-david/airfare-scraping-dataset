{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airfare Scraping\n",
    "Developing and cleaning a Kaggle-ready dataset for advanced analysis on airline flight information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "1. Headless Selenium testing with Python and PhantomJS: <br>\n",
    "https://realpython.com/blog/python/headless-selenium-testing-with-python-and-phantomjs/\n",
    "2. Setting PhantomJS user agent string: <br>\n",
    "https://coderwall.com/p/9jgaeq/set-phantomjs-user-agent-string\n",
    "3. Another helpful reference for airfare scraping: <br>\n",
    "https://github.com/hakanmhmd/air-fare-scraper/blob/master/flight_price_scrape.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /anaconda/lib/python3.5/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWarning:\u001b[0m phantomjs 2.1.1 is already installed\r\n"
     ]
    }
   ],
   "source": [
    "!brew install PhantomJS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "import requests\n",
    "import bs4 as bs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_flights_to_pandas(google_flights_url):\n",
    "    '''\n",
    "    Creates a table with columns for airline, departure/arrival city, \n",
    "    departure/arrival time, price, duration, number of stops (or nonstop)\n",
    "    \n",
    "    Input:\n",
    "        google_flights_url (string): Google Flights url to scrape (see example below)\n",
    "    Example Input:\n",
    "        https://www.google.com/flights/#search;f=SFO;t=EWR;d=2018-04-01;tt=o;a=UA;s=0\n",
    "        f = from this airport (ex. SFO)\n",
    "        t = to this airport (ex. JFK)\n",
    "        d = date of flight (ex. yyyy-mm-dd)\n",
    "        tt = travel type (ex. o for one-way, m for multi-city)\n",
    "        a = airline (ex. UA for United)\n",
    "        s = # of stops (ex. 0 for nonstop)\n",
    "    Output:\n",
    "        google_flights_pd (pandas DataFrame): Airfare information including \n",
    "            date, airline, price, duration, and number of stops\n",
    "    '''\n",
    "    assert requests.get(url).status_code == 200\n",
    "\n",
    "    dcap = dict(DesiredCapabilities.PHANTOMJS)\n",
    "    dcap[\"phantomjs.page.settings.userAgent\"] = (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94\")\n",
    "\n",
    "    driver = webdriver.PhantomJS(desired_capabilities=dcap)\n",
    "    driver.get(google_flights_url)\n",
    "    \n",
    "    # Read in source.content to beautifulsoup \n",
    "    # we pass in the source content and choose a parser\n",
    "    soup = bs.BeautifulSoup(driver.page_source, 'lxml') \n",
    "\n",
    "    # the airline is divided by <div class=\"LJV2HGB-d-j\">\n",
    "    airlines = []\n",
    "    for airline in soup.findAll('div','LJV2HGB-d-j'):\n",
    "        airlines.append(airline.text)\n",
    "    \n",
    "    # scrape the departure and arrival city and date\n",
    "    size = len(soup.findAll('div','LJV2HGB-d-j'))\n",
    "    departure_city = []\n",
    "    arrival_city = []\n",
    "    date = []\n",
    "    for i in range(size):\n",
    "        departure_index = google_flights_url.find(\"f=\")\n",
    "        departure_city.append(google_flights_url[departure_index + 2: departure_index + 5])\n",
    "        arrival_index = google_flights_url.find(\"t=\")\n",
    "        arrival_city.append(google_flights_url[arrival_index + 2: arrival_index + 5])\n",
    "        date_index = google_flights_url.find(\"d=\")\n",
    "        date.append(google_flights_url[date_index + 2: date_index + 12])\n",
    "        \n",
    "    # time of departure/arrival is divided by <div class=\"LJV2HGB-d-Zb\">\n",
    "    departure_times = []\n",
    "    arrival_times = []\n",
    "    for time in soup.findAll('div', 'LJV2HGB-d-Zb'):\n",
    "        time = time.text.split(' â€“ ')\n",
    "        departure_times.append(time[0])\n",
    "        arrival_times.append(time[1])\n",
    "    \n",
    "    # the price is between <div class=\"LJV2HGB-d-Ab\">\n",
    "    prices = []\n",
    "    for price in soup.findAll('div', 'LJV2HGB-d-Ab'):\n",
    "        prices.append(price.text.replace('$','').replace(',',''))\n",
    "    \n",
    "    # length of the flight is divided by <div class=\"LJV2HGB-d-E\">\n",
    "    duration = []\n",
    "    for time in soup.findAll('div', 'LJV2HGB-d-E'):\n",
    "        duration.append(time.text)\n",
    "       \n",
    "    # the number of stops is divided by <div class=\"LJV2HGB-d-Qb\">\n",
    "    stops = []\n",
    "    for stop in soup.findAll('div', 'LJV2HGB-d-Qb'):\n",
    "        stops.append(stop.text)\n",
    "    \n",
    "    return pd.DataFrame(list(zip(airlines, departure_city, arrival_city, date, \n",
    "                                 departure_times, arrival_times, prices,duration, stops)),\n",
    "                      columns = ['Airline', 'Departure City', 'Arrival City', 'Date', \n",
    "                                 'Departure Time', 'Arrival Time', 'Price', 'Duration', 'Stops'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the Google Flights Links to Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "url_list.append(\"https://www.google.com/flights/#search;f=SFO;t=EWR;d=2018-04-01;tt=o;a=UA;s=0\")\n",
    "url_list.append(\"https://www.google.com/flights/#search;f=SFO;t=HNL;d=2018-04-01;tt=o;a=UA;s=0\")\n",
    "\n",
    "df_list = []\n",
    "for url in url_list:\n",
    "    df_list.append(google_flights_to_pandas(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    file_path = str(i+1) + \".csv\"\n",
    "    df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
